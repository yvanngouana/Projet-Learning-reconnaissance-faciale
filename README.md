# Projet de Reconnaissance de Visages - Groupe 5
## MDSMS1 - Deep Learning

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.15-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-Academic-green.svg)]()

### Membres du groupe
- **NGOUANA FAUMETE Etienne**
- **BEINDI FÃ‰LIX HOUMBI**
- **SCHOUAME Jean Pierre**
- **DEDIM GUELBE APPOLINAIRE**

---

## ğŸ“‹ Description du projet

Ce projet implÃ©mente un systÃ¨me de **reconnaissance de visages** pour identifier automatiquement les Ã©tudiants de la promotion MDSMS1. Le modÃ¨le utilise le **deep learning** avec une approche de **transfer learning** basÃ©e sur MobileNetV2 pour atteindre une haute prÃ©cision sur le jeu de test.

### Objectifs
1. Collecter et prÃ©parer un dataset de visages d'Ã©tudiants
2. PrÃ©-traiter les images pour la reconnaissance faciale
3. EntraÃ®ner un modÃ¨le de deep learning performant
4. DÃ©ployer le modÃ¨le pour des prÃ©dictions en temps rÃ©el

---

## ğŸ“š Table des matiÃ¨res
1. [Collecte des donnÃ©es](#collecte-des-donnÃ©es)
2. [PrÃ©-traitement](#prÃ©-traitement)
3. [Architecture du modÃ¨le](#architecture-du-modÃ¨le)
4. [EntraÃ®nement](#entraÃ®nement)
5. [RÃ©sultats](#rÃ©sultats)
6. [Installation](#installation)
7. [Utilisation](#utilisation)
8. [Structure du projet](#structure-du-projet)
9. [AmÃ©liorations futures](#amÃ©liorations-futures)

---

## ğŸ“Š Collecte des donnÃ©es

### MÃ©thodologie
- **Nombre d'Ã©tudiants:** ~20 Ã©tudiants de MDSMS1
- **Images par Ã©tudiant:** 10-20 images variÃ©es
- **Total d'images:** ~200+ images
- **Conditions variÃ©es:**
  - DiffÃ©rents angles (face, 3/4, profil)
  - DiffÃ©rents Ã©clairages (naturel, artificiel)
  - DiffÃ©rentes expressions (neutre, sourire)
  - Avec/sans lunettes (si applicable)

### Organisation des donnÃ©es
Les donnÃ©es ont Ã©tÃ© collectÃ©es de maniÃ¨re collaborative avec l'ensemble de la promotion pour garantir la cohÃ©rence et la qualitÃ© du dataset.

```
Data/
â”œâ”€â”€ NGOUANA/
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â”œâ”€â”€ image_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ BEINDI/
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

---

## ğŸ”§ PrÃ©-traitement

### Ã‰tapes appliquÃ©es

1. **DÃ©tection des visages**
   - Algorithme: **MTCNN** (Multi-task Cascaded Convolutional Networks)
   - Extraction et recadrage automatique des visages
   - DÃ©tection robuste mÃªme avec variations d'angle et d'Ã©clairage

2. **Redimensionnement**
   - Taille cible: **160x160 pixels**
   - Maintien du ratio d'aspect avec marge de 20%

3. **Normalisation**
   - Pixels normalisÃ©s entre 0 et 1
   - Formule: `pixel_value / 255.0`

4. **Augmentation des donnÃ©es** (Data Augmentation)
   - Rotation: Â±20Â°
   - DÃ©calage horizontal/vertical: Â±20%
   - Flip horizontal
   - Zoom: Â±20%
   - Ajustement de luminositÃ©: 80-120%

---

## ğŸ—ï¸ Architecture du modÃ¨le

### Approche: Transfer Learning

Nous avons utilisÃ© **MobileNetV2** prÃ©-entraÃ®nÃ© sur ImageNet comme modÃ¨le de base, avec des couches personnalisÃ©es pour la classification des Ã©tudiants.

### Structure du modÃ¨le

```
Input (160x160x3)
    â†“
MobileNetV2 (prÃ©-entraÃ®nÃ© sur ImageNet)
    â†“
GlobalAveragePooling2D
    â†“
Dense(256, activation='relu')
    â†“
Dropout(0.5)
    â†“
Dense(128, activation='relu')
    â†“
Dropout(0.3)
    â†“
Dense(num_classes, activation='softmax')
```

### Justification du choix

- **MobileNetV2:** LÃ©ger et efficace, idÃ©al pour le dÃ©ploiement
- **Transfer Learning:** Performances Ã©levÃ©es mÃªme avec un petit dataset
- **Architecture simple:** Ã‰vite l'overfitting grÃ¢ce aux couches Dropout

---

## ğŸ¯ EntraÃ®nement

### HyperparamÃ¨tres

- **Optimizer:** Adam
- **Learning rate initial:** 0.001
- **Learning rate fine-tuning:** 0.0001
- **Batch size:** 32
- **Loss function:** Categorical Crossentropy
- **Epochs phase 1:** 20 (couches de base gelÃ©es)
- **Epochs phase 2:** 30 (fine-tuning complet)

### Callbacks utilisÃ©s

1. **EarlyStopping**
   - Monitor: `val_loss`
   - Patience: 10 epochs
   - Restaure les meilleurs poids

2. **ModelCheckpoint**
   - Sauvegarde du meilleur modÃ¨le basÃ© sur `val_accuracy`

3. **ReduceLROnPlateau**
   - RÃ©duction du learning rate si plateau dÃ©tectÃ©
   - Factor: 0.5

### StratÃ©gie d'entraÃ®nement en 2 phases

1. **Phase 1:** EntraÃ®nement des couches personnalisÃ©es uniquement (couches de base gelÃ©es)
   - Permet d'apprendre les caractÃ©ristiques spÃ©cifiques aux visages des Ã©tudiants

2. **Phase 2:** Fine-tuning de toutes les couches avec un learning rate rÃ©duit
   - Affine le modÃ¨le pour amÃ©liorer les performances

### SÃ©paration des donnÃ©es

- **Training set:** 70% des donnÃ©es
- **Validation set:** 15% des donnÃ©es
- **Test set:** 15% des donnÃ©es

---

## ğŸ“ˆ RÃ©sultats

### Performance du modÃ¨le

- **Accuracy (Train):** 71.37%
- **Accuracy (Validation):** 71.15%
- **Accuracy (Test):** 57.69%
- **Loss (Test):** 1.2307

### Statistiques du dataset

- **Nombre de classes:** 19 Ã©tudiants
- **Total d'images:** 345 images
- **Training set:** 241 images (70%)
- **Validation set:** 52 images (15%)
- **Test set:** 52 images (15%)

### Graphiques

Les graphiques d'entraÃ®nement et la matrice de confusion sont gÃ©nÃ©rÃ©s automatiquement dans le notebook et sauvegardÃ©s dans le dossier `models/`.

- `training_history.png` - Ã‰volution de l'accuracy et de la loss
- `confusion_matrix.png` - Matrice de confusion sur le jeu de test

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip
- (Optionnel) GPU avec CUDA pour accÃ©lÃ©rer l'entraÃ®nement

### Installation des dÃ©pendances

```bash
# Cloner ou tÃ©lÃ©charger le projet
cd projet-reconnaissance-visages

# CrÃ©er un environnement virtuel (recommandÃ©)
python -m venv venv

# Activer l'environnement virtuel
# Sur Linux/Mac:
source venv/bin/activate
# Sur Windows:
venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

---

## ğŸ’» Utilisation

### 1. EntraÃ®ner le modÃ¨le

Ouvrez le notebook Jupyter et exÃ©cutez toutes les cellules:

```bash
jupyter notebook Face_Recognition_Project.ipynb
```

Le notebook vous guidera Ã  travers toutes les Ã©tapes:
- Exploration des donnÃ©es
- PrÃ©-traitement
- EntraÃ®nement
- Ã‰valuation
- Sauvegarde du modÃ¨le

### 2. Tester le modÃ¨le sur une image

```bash
python inference.py path/to/image.jpg
```

**Sortie attendue:**
```
RÃ©sultats de la prÃ©diction:
1. NGOUANA FAUMETE Etienne: 95.67%
2. SCHOUAME Jean Pierre: 3.21%
3. BEINDI MEMANG HOUMBI: 0.89%
```

### 3. Tester avec la webcam

```bash
python inference.py --webcam
```

Appuyez sur 'q' pour quitter le mode webcam.

### 4. Traiter un lot d'images

```bash
python inference.py --batch ./test_images
```

### 5. Options avancÃ©es

```bash
# Afficher les 5 meilleures prÃ©dictions
python inference.py image.jpg --top-k 5

# Utiliser un modÃ¨le personnalisÃ©
python inference.py image.jpg --model custom_model.h5

# Afficher l'aide
python inference.py --help
```

### 6. Charger le modÃ¨le dans votre code

```python
from tensorflow.keras.models import load_model
import pickle
import json
import numpy as np
import cv2

# Charger le modÃ¨le
model = load_model('models/face_recognition_model.h5')

# Charger le label encoder
with open('models/label_encoder.pkl', 'rb') as f:
    label_encoder = pickle.load(f)

# Charger une image et prÃ©dire
img = cv2.imread('test_image.jpg')
img_resized = cv2.resize(img, (160, 160))
img_normalized = img_resized / 255.0
img_batch = np.expand_dims(img_normalized, axis=0)

# PrÃ©diction
predictions = model.predict(img_batch)[0]
best_idx = np.argmax(predictions)
student_name = label_encoder.inverse_transform([best_idx])[0]
confidence = predictions[best_idx] * 100

print(f"PrÃ©diction: {student_name} ({confidence:.2f}%)")
```

---

## ğŸ“ Structure du projet

```
projet-reconnaissance-visages/
â”œâ”€â”€ Data/                                # Images brutes collectÃ©es
â”‚   â”œâ”€â”€ NGOUANA/
â”‚   â”œâ”€â”€ BEINDI/
â”‚   â”œâ”€â”€ SCHOUAME/
â”‚   â”œâ”€â”€ DEDIM/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ processed_data/                      # Images prÃ©-traitÃ©es (gÃ©nÃ©rÃ©es)
â”‚   â”œâ”€â”€ NGOUANA/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ models/                              # ModÃ¨les et artefacts (gÃ©nÃ©rÃ©s)
â”‚   â”œâ”€â”€ face_recognition_model.h5        # ModÃ¨le entraÃ®nÃ© final
â”‚   â”œâ”€â”€ best_model_phase1.h5             # Meilleur modÃ¨le phase 1
â”‚   â”œâ”€â”€ best_model_phase2.h5             # Meilleur modÃ¨le phase 2
â”‚   â”œâ”€â”€ label_encoder.pkl                # Encodeur des labels
â”‚   â”œâ”€â”€ model_config.json                # Configuration du modÃ¨le
â”‚   â”œâ”€â”€ confusion_matrix.png             # Matrice de confusion
â”‚   â””â”€â”€ training_history.png             # Graphiques d'entraÃ®nement
â”‚
â”œâ”€â”€ Face_Recognition_Project.ipynb       # Notebook principal
â”œâ”€â”€ inference.py                         # Script d'infÃ©rence
â”œâ”€â”€ requirements.txt                     # DÃ©pendances Python
â”œâ”€â”€ README.md                            # Ce fichier
â”œâ”€â”€ train.csv                            # Dataset d'entraÃ®nement (gÃ©nÃ©rÃ©)
â”œâ”€â”€ val.csv                              # Dataset de validation (gÃ©nÃ©rÃ©)
â””â”€â”€ test.csv                             # Dataset de test (gÃ©nÃ©rÃ©)
```

---

## ğŸ”¬ DÃ©tails techniques

### Technologies utilisÃ©es

- **TensorFlow/Keras:** Framework de deep learning
- **OpenCV:** Traitement d'images
- **MTCNN:** DÃ©tection de visages
- **NumPy/Pandas:** Manipulation de donnÃ©es
- **Matplotlib/Seaborn:** Visualisations
- **Scikit-learn:** PrÃ©traitement et mÃ©triques

### Configuration matÃ©rielle recommandÃ©e

- **CPU:** Intel Core i5 ou Ã©quivalent
- **RAM:** 8 GB minimum
- **GPU:** NVIDIA GPU avec CUDA (optionnel mais recommandÃ©)
- **Espace disque:** 2 GB minimum

---

## âš ï¸ Limitations et considÃ©rations

### Limitations actuelles

1. **Taille du dataset:** Performance limitÃ©e par le nombre d'images par personne
2. **Conditions d'Ã©clairage:** Peut Ãªtre moins prÃ©cis dans des conditions extrÃªmes
3. **Angles de vue:** Meilleure performance avec des visages de face
4. **Occlusions:** DifficultÃ© avec masques, lunettes de soleil, etc.

### ConsidÃ©rations Ã©thiques

- Ce modÃ¨le est dÃ©veloppÃ© Ã  des fins **Ã©ducatives uniquement**
- Respect de la vie privÃ©e et consentement des participants
- Ne pas utiliser Ã  des fins de surveillance non autorisÃ©e

---

## ğŸš§ AmÃ©liorations futures

### Court terme

- [ ] Augmenter le dataset avec plus d'images variÃ©es
- [ ] ImplÃ©menter la reconnaissance de visages multiples
- [ ] Optimiser les performances en temps rÃ©el

### Moyen terme

- [ ] Tester d'autres architectures (VGGFace, FaceNet, ArcFace)
- [ ] ImplÃ©menter un systÃ¨me de seuil de confiance
- [ ] Ajouter la dÃ©tection d'Ã©motions

### Long terme

- [ ] CrÃ©er une interface web avec Flask/Streamlit
- [ ] DÃ©ployer sur mobile (TensorFlow Lite)
- [ ] ImplÃ©menter un systÃ¨me d'apprentissage continu

---

## ğŸ› DÃ©pannage

### ProblÃ¨mes courants

**1. Erreur "No module named 'tensorflow'"**
```bash
pip install tensorflow
```

**2. Erreur avec MTCNN**
```bash
pip install mtcnn
```

**3. ProblÃ¨mes de webcam**
- VÃ©rifiez que votre webcam est connectÃ©e
- Essayez de changer l'indice: `cv2.VideoCapture(1)` au lieu de `0`

**4. Erreur de mÃ©moire GPU**
- RÃ©duisez le batch size dans le notebook
- Utilisez le CPU en ajoutant:
  ```python
  import os
  os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
  ```

---

## ğŸ“– RÃ©fÃ©rences

### Articles scientifiques

- **MobileNetV2:** [Sandler et al., 2018](https://arxiv.org/abs/1801.04381)
- **MTCNN:** [Zhang et al., 2016](https://arxiv.org/abs/1604.02878)
- **Transfer Learning:** [Yosinski et al., 2014](https://arxiv.org/abs/1411.1792)

### Documentation

- [TensorFlow Documentation](https://www.tensorflow.org/)
- [Keras Applications](https://keras.io/api/applications/)
- [OpenCV Documentation](https://docs.opencv.org/)
- [MTCNN GitHub](https://github.com/ipazc/mtcnn)

### Ressources d'apprentissage

- [Deep Learning Specialization - Andrew Ng](https://www.coursera.org/specializations/deep-learning)
- [Face Recognition Guide](https://machinelearningmastery.com/how-to-develop-a-face-recognition-system-using-facenet-in-keras/)

---

## ğŸ“ Historique des versions

### Version 1.0 (Octobre 2025)
- ImplÃ©mentation initiale du projet
- MobileNetV2 avec transfer learning
- Script d'infÃ©rence complet
- Documentation complÃ¨te

---

## ğŸ‘¥ Contribution

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du cours de Deep Learning - MDSMS1.

### RÃ©partition des tÃ¢ches

- **NGOUANA FAUMETE Etienne:** Configuration, entraÃ®nement, documentation technique
- **BEINDI FÃ‰LIX HOUMBI:** PrÃ©-traitement, dÃ©tection de visages
- **SCHOUAME Jean Pierre:** Organisation des donnÃ©es, visualisations
- **DEDIM GUELBE APPOLINAIRE:** Script d'infÃ©rence, tests, documentation utilisateur

---

## ğŸ“§ Contact

Pour toute question concernant ce projet, contactez les membres du Groupe 5.

---

## ğŸ“„ Licence

Ce projet est rÃ©alisÃ© dans le cadre du cours de Deep Learning - MDSMS1.
**Usage acadÃ©mique uniquement.**

---

## ğŸ™ Remerciements

- Professeur et Ã©quipe pÃ©dagogique de MDSMS1
- Tous les Ã©tudiants ayant participÃ© Ã  la collecte de donnÃ©es
- CommunautÃ© TensorFlow et Keras

---

**DÃ©veloppÃ© avec â¤ï¸ par le Groupe 5 - MDSMS1**

*DerniÃ¨re mise Ã  jour: Octobre 2025*
